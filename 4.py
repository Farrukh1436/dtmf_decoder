#!/usr/bin/env python3
"""
dtmf_decoder.py

DTMF (Dual-Tone Multi-Frequency) decoder that:
- Loads a WAV file
- Segments it into active tones using short-time energy
- Uses FFT to detect the pair of frequencies per tone
- Maps frequency pairs to standard DTMF symbols (0-9, A-D, *, #)

Usage:
    python dtmf_decoder.py /path/to/file.wav

Optional arguments:
    --tol TOL           Frequency tolerance in Hz (default: 4.0)
    --min-duration S    Minimum tone duration (s) to consider (default: 0.05)
    --energy-thresh F   Relative energy threshold (0..1) (default: 0.08)
    --quiet             Reduce console output
    --digits-only       Filter output to digits 0-9 only

Dependencies:
    numpy, scipy

Author: Generated by ChatGPT (adapted for production use)
"""
from __future__ import annotations

import argparse
import math
import os
import sys
from dataclasses import dataclass
from typing import List, Optional, Tuple

import numpy as np
from scipy.io import wavfile
from scipy.signal import windows

# ---------------------
# DTMF frequency table
# ---------------------
DTMF_LOW_FREQS = np.array([697.0, 770.0, 852.0, 941.0])
DTMF_HIGH_FREQS = np.array([1209.0, 1336.0, 1477.0, 1633.0])

DTMF_MAP = {
    (697.0, 1209.0): "1",
    (697.0, 1336.0): "2",
    (697.0, 1477.0): "3",
    (697.0, 1633.0): "A",
    (770.0, 1209.0): "4",
    (770.0, 1336.0): "5",
    (770.0, 1477.0): "6",
    (770.0, 1633.0): "B",
    (852.0, 1209.0): "7",
    (852.0, 1336.0): "8",
    (852.0, 1477.0): "9",
    (852.0, 1633.0): "C",
    (941.0, 1209.0): "*",
    (941.0, 1336.0): "0",
    (941.0, 1477.0): "#",
    (941.0, 1633.0): "D",
}


@dataclass
class ToneDetection:
    start_s: float
    end_s: float
    low_freq: Optional[float]
    high_freq: Optional[float]
    symbol: Optional[str]
    confidence: float


# ---------------------
# Helper functions
# ---------------------
def _next_pow2(n: int) -> int:
    """Return next power of two >= n."""
    return 1 << (n - 1).bit_length()


def _normalize_audio(data: np.ndarray) -> np.ndarray:
    """Normalize PCM audio to float32 in [-1, 1]."""
    if data.dtype == np.float32 or data.dtype == np.float64:
        audio = data.astype(np.float32)
        # If already in -1..1 assume it's fine; clip to be safe
        audio = np.clip(audio, -1.0, 1.0)
    else:
        # integer types
        max_val = float(np.iinfo(data.dtype).max)
        min_val = float(np.iinfo(data.dtype).min)
        # Use symmetric scaling by max(abs(min), max)
        scale = max(abs(min_val), abs(max_val))
        if scale == 0:
            raise ValueError("Audio data has zero dynamic range.")
        audio = data.astype(np.float32) / scale
    return audio


def load_wav_mono(path: str) -> Tuple[int, np.ndarray]:
    """
    Load a WAV file and return sample rate and mono audio (float32).
    Raises FileNotFoundError or ValueError for bad input.
    """
    if not os.path.isfile(path):
        raise FileNotFoundError(f"Audio file not found: {path}")
    sr, data = wavfile.read(path)
    if data.size == 0:
        raise ValueError("Empty audio file.")
    audio = _normalize_audio(data)
    # Convert to mono
    if audio.ndim > 1:
        audio = np.mean(audio, axis=1)
    return int(sr), audio


def short_time_energy(
    audio: np.ndarray, sr: int, frame_ms: float = 10.0, hop_ms: Optional[float] = None
) -> Tuple[np.ndarray, int]:
    """
    Compute short-time energy (RMS) frames.
    Returns tuple (energy_array, frame_length_samples).
    """
    frame_len = max(1, int(sr * (frame_ms / 1000.0)))
    if hop_ms is None:
        hop = frame_len // 2
    else:
        hop = max(1, int(sr * (hop_ms / 1000.0)))
    # Pad end so all frames fit
    n_frames = 1 + (len(audio) - frame_len) // hop if len(audio) >= frame_len else 1
    energy = []
    for i in range(n_frames):
        start = i * hop
        end = start + frame_len
        frame = audio[start:end]
        if frame.size == 0:
            e = 0.0
        else:
            e = float(np.sqrt(np.mean(frame * frame)))
        energy.append(e)
    return np.array(energy), hop


def detect_active_segments(
    audio: np.ndarray,
    sr: int,
    energy_thresh: float = 0.08,
    min_silence_gap: float = 0.05,
    min_duration: float = 0.04,
) -> List[Tuple[int, int]]:
    """
    Detect active segments (start_sample, end_sample) using short-time energy.
    - energy_thresh: relative to max RMS (0..1)
    - min_silence_gap: merge segments separated by less than this (seconds)
    - min_duration: drop segments shorter than this (seconds)
    """
    energy, hop = short_time_energy(audio, sr, frame_ms=10.0)
    if energy.size == 0:
        return []
    max_e = float(np.max(energy))
    if max_e <= 0:
        return []

    thresh = max(1e-8, energy_thresh * max_e)
    active_frames = np.where(energy >= thresh)[0]
    if active_frames.size == 0:
        return []

    # Convert frame indices to sample indices
    frame_len = max(1, int(sr * (10.0 / 1000.0)))  # matches short_time_energy default
    # Group contiguous frames
    segments = []
    start_frame = active_frames[0]
    prev_frame = active_frames[0]
    for f in active_frames[1:]:
        if f - prev_frame > 1:
            # break
            s = int(start_frame * hop)
            e = int((prev_frame * hop) + frame_len)
            segments.append((s, e))
            start_frame = f
        prev_frame = f
    # last
    s = int(start_frame * hop)
    e = int((prev_frame * hop) + frame_len)
    segments.append((s, e))

    # Merge segments separated by small gaps
    merged = []
    min_gap_frames = int(math.ceil(min_silence_gap * sr / hop))
    cur_s, cur_e = segments[0]
    for s, e in segments[1:]:
        gap_samples = s - cur_e
        if gap_samples <= min_silence_gap * sr:
            # merge
            cur_e = e
        else:
            merged.append((cur_s, cur_e))
            cur_s, cur_e = s, e
    merged.append((cur_s, cur_e))

    # Filter by min_duration
    min_samples = int(max(1, min_duration * sr))
    filtered = []
    for s, e in merged:
        if e - s >= min_samples:
            # clamp to audio bounds
            s_clamp = max(0, s)
            e_clamp = min(len(audio), e)
            filtered.append((s_clamp, e_clamp))
    return filtered


def _find_peak_in_band(
    freqs: np.ndarray, mags: np.ndarray, band: Tuple[float, float]
) -> Tuple[Optional[float], float]:
    """
    Find the frequency (Hz) with maximum magnitude within band (low, high).
    Returns (frequency or None, normalized magnitude 0..1).
    """
    low, high = band
    idx = np.where((freqs >= low) & (freqs <= high))[0]
    if idx.size == 0:
        return None, 0.0
    band_mags = mags[idx]
    max_idx = idx[int(np.argmax(band_mags))]
    peak_freq = float(freqs[max_idx])
    mag = float(mags[max_idx])
    # Normalize magnitude relative to max of the full spectrum to give a confidence measure
    mag_norm = mag / (np.max(mags) + 1e-12)
    return peak_freq, mag_norm


def map_to_dtmf(low_f: float, high_f: float, tol: float = 4.0) -> Optional[str]:
    """
    Map low and high detected frequencies to nearest DTMF standard frequencies.
    If either is outside tolerance, returns None.
    """
    # Find nearest standard low and high
    idx_low = int(np.argmin(np.abs(DTMF_LOW_FREQS - low_f)))
    idx_high = int(np.argmin(np.abs(DTMF_HIGH_FREQS - high_f)))
    chosen_low = float(DTMF_LOW_FREQS[idx_low])
    chosen_high = float(DTMF_HIGH_FREQS[idx_high])
    if abs(chosen_low - low_f) <= tol and abs(chosen_high - high_f) <= tol:
        return DTMF_MAP.get((chosen_low, chosen_high))
    return None


def analyze_segment(
    seg_audio: np.ndarray,
    sr: int,
    tol: float = 4.0,
    min_fft_size: int = 4096,
) -> Tuple[Optional[float], Optional[float], float]:
    """
    Analyze a single tone segment to detect low & high frequency peaks.
    Returns (low_freq, high_freq, confidence_score)
    """
    n = seg_audio.shape[0]
    if n < 1:
        return None, None, 0.0
    # Window
    win = windows.hamming(n, sym=False)
    x = seg_audio * win
    # Zero pad to at least min_fft_size or next pow2 for resolution
    fft_n = max(min_fft_size, _next_pow2(n))
    X = np.fft.rfft(x, n=fft_n)
    mags = np.abs(X)
    freqs = np.fft.rfftfreq(fft_n, d=1.0 / sr)
    # Consider only positive magnitudes
    # Find peaks in low and high DTMF bands (wider margins)
    low_band = (600.0, 1000.0)
    high_band = (1150.0, 1700.0)
    low_peak, low_conf = _find_peak_in_band(freqs, mags, low_band)
    high_peak, high_conf = _find_peak_in_band(freqs, mags, high_band)

    # Confidence aggregated
    confidence = float((low_conf + high_conf) / 2.0)
    return low_peak, high_peak, confidence


def decode_file(
    path: str,
    tol: float = 4.0,
    energy_thresh: float = 0.08,
    min_duration: float = 0.04,
    quiet: bool = False,
) -> Tuple[str, List[ToneDetection]]:
    """
    Decode DTMF tones from a WAV file.
    Returns (decoded_string, list_of_ToneDetection).
    """
    sr, audio = load_wav_mono(path)
    if not quiet:
        print(f"Loaded '{path}' -- sample rate {sr} Hz, {len(audio)} samples ({len(audio)/sr:.2f} s)")

    segments = detect_active_segments(
        audio,
        sr,
        energy_thresh=energy_thresh,
        min_silence_gap=0.04,
        min_duration=min_duration,
    )
    if not segments:
        raise ValueError("No active DTMF segments detected (check energy threshold or audio).")

    detections: List[ToneDetection] = []
    decoded_chars: List[str] = []

    for s, e in segments:
        seg = audio[s:e]
        # Ensure segment is minimum length for frequency resolution: pad if too short
        # We'll zero-pad inside analyze_segment via FFT sizing
        low_f, high_f, conf = analyze_segment(seg, sr, tol=tol, min_fft_size=8192)
        symbol = None
        if low_f is not None and high_f is not None:
            symbol = map_to_dtmf(low_f, high_f, tol=tol)
        # Convert sample indices to seconds
        start_s = s / sr
        end_s = e / sr
        detections.append(ToneDetection(start_s, end_s, low_f, high_f, symbol, conf))
        if symbol is None:
            decoded_chars.append("?")
        else:
            decoded_chars.append(symbol)
        if not quiet:
            print(
                f"Segment {start_s:.3f}-{end_s:.3f}s: low={low_f and low_f:.1f}Hz, "
                f"high={high_f and high_f:.1f}Hz, sym={symbol}, conf={conf:.2f}"
            )

    decoded = "".join(decoded_chars)
    return decoded, detections


# ---------------------
# CLI Entrypoint
# ---------------------
def main(argv: Optional[List[str]] = None) -> int:
    parser = argparse.ArgumentParser(description="DTMF decoder using FFT-based detection.")
    parser.add_argument("wavfile", help="Path to WAV file to analyze.")
    parser.add_argument(
        "--tol",
        type=float,
        default=4.0,
        help="Frequency tolerance in Hz for mapping to DTMF standards (default: 4.0)",
    )
    parser.add_argument(
        "--energy-thresh",
        type=float,
        default=0.08,
        help="Relative short-time energy threshold (0..1) to detect active frames (default: 0.08)",
    )
    parser.add_argument(
        "--min-duration",
        type=float,
        default=0.04,
        help="Minimum tone duration in seconds to consider (default: 0.04)",
    )
    parser.add_argument(
        "--quiet", action="store_true", help="Suppress per-segment output (only print final code)."
    )
    parser.add_argument(
        "--digits-only",
        action="store_true",
        help="Filter output to digits 0-9 only (remove A-D and * and #).",
    )
    args = parser.parse_args(argv)

    try:
        decoded, detections = decode_file(
            args.wavfile,
            tol=args.tol,
            energy_thresh=args.energy_thresh,
            min_duration=args.min_duration,
            quiet=args.quiet,
        )
    except Exception as exc:
        print(f"Error: {exc}", file=sys.stderr)
        return 2

    if args.digits_only:
        filtered = "".join([c for c in decoded if c.isdigit()])
        print(filtered)
    else:
        print(decoded)

    return 0


if __name__ == "__main__":
    raise SystemExit(main())